{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qdrow9INunBr"
   },
   "outputs": [],
   "source": [
    "from psaw import PushshiftAPI\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from sys import platform\n",
    "api = PushshiftAPI()\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Easy switching between Windows and Mac directories\n",
    "if platform == 'darwin':\n",
    "    ticker = pd.read_csv(r'/Users/nishaldave/OneDrive - University of Bristol/Dissertation/Volume Forecasting/Data/WSB Scraper/tickers.csv',header=None)\n",
    "    ticker[1] = '$'+ticker[0]\n",
    "    ticker.to_csv(r'/Users/nishaldave/OneDrive - University of Bristol/Dissertation/Volume Forecasting/Data/WSB Scraper/tickers_new.csv', index=False)\n",
    "    import csv\n",
    "    with open(r'/Users/nishaldave/OneDrive - University of Bristol/Dissertation/Volume Forecasting/Data/WSB Scraper/tickers_new.csv') as csv_file:\n",
    "        reader = csv.reader(csv_file, delimiter=',')\n",
    "        tickers = list(reader)\n",
    "    del tickers[0]\n",
    "    \n",
    "elif platform =='win32':\n",
    "    ticker = pd.read_csv(r'C:\\Users\\Nish\\OneDrive - University of Bristol\\Dissertation\\Volume Forecasting\\Data\\WSB Scraper\\tickers.csv',header=None)\n",
    "    ticker[1] = '$'+ticker[0]\n",
    "    ticker.to_csv(r'C:\\Users\\Nish\\OneDrive - University of Bristol\\Dissertation\\Volume Forecasting\\Data\\WSB Scraper\\tickers_new.csv', index=False)\n",
    "    import csv\n",
    "    with open(r'C:\\Users\\Nish\\OneDrive - University of Bristol\\Dissertation\\Volume Forecasting\\Data\\WSB Scraper\\tickers_new.csv') as csv_file:\n",
    "        reader = csv.reader(csv_file, delimiter=',')\n",
    "        tickers = list(reader)\n",
    "    del tickers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q_Im7pMXu7do",
    "outputId": "d13e88d9-abcd-477d-c98a-3b973d3b8396"
   },
   "outputs": [],
   "source": [
    "#Scrapes all reddit submissions associated with each ticker \n",
    "for i in tickers:\n",
    "    start_date = int(dt.datetime(2020, 6, 29, 0,0,0).timestamp())\n",
    "    start_date2 = int(dt.datetime(2020, 6, 29, 23,59,59).timestamp())\n",
    "    end_date = int(dt.datetime(2021, 6, 25, 0,0,0).timestamp())\n",
    "\n",
    "    gen_init = api.search_submissions(subreddit='wallstreetbets',q=i,limit=10000,after=start_date,before=start_date2)\n",
    "    df_init = pd.DataFrame([X.d_ for X in gen_init])\n",
    "    df_final = df_init[0:0]\n",
    "\n",
    "    while start_date < end_date:    \n",
    "      gen_data = api.search_submissions(subreddit='wallstreetbets',q=i,limit=10000,after=start_date,before=start_date2)\n",
    "      df_inter = pd.DataFrame([X.d_ for X in gen_data])\n",
    "      df_final = pd.concat([df_final,df_inter],axis=0)\n",
    "      #df_final['created_utc'] = pd.to_datetime(df_final['created_utc'],unit='s')\n",
    "      df_final['asset'] = i[0]\n",
    "      start_date  = start_date  + 86400\n",
    "      start_date2 = start_date2 + 86400\n",
    "      \n",
    "    dfs.append(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Necessary variables to retain\n",
    "#created_utc, link_flair_css_class, num_comments, selftext, title, asset.\n",
    "for i in range(len(dfs)):\n",
    "    dfs[i] = dfs[i][['created_utc','link_flair_css_class','num_comments','selftext','title','asset']]\n",
    "    dfs[i]['created_utc'] = pd.to_datetime(dfs[i]['created_utc'],unit='s')\n",
    "    dfs[i] = dfs[i].sort_values(by='created_utc',ascending=True)\n",
    "    dfs[i] = dfs[i][dfs[i]['selftext'] != '[removed]']\n",
    "    dfs[i] = dfs[i][dfs[i]['selftext'] != '[deleted]']\n",
    "    dfs[i] = dfs[i][dfs[i]['selftext'].notna()]\n",
    "    dfs[i] = dfs[i][dfs[i]['selftext']!=''] #drop selftexts where there is no valid description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G0rOdRHA2VkP"
   },
   "outputs": [],
   "source": [
    "pd.concat(dfs).to_csv('C:\\Users\\Nish\\OneDrive - University of Bristol\\Dissertation\\Volume Forecasting\\Data\\WSB Scraper\\Submissions\\assets_v1s.csv')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
